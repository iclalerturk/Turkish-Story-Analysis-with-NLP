from collections import Counter, defaultdict
from difflib import get_close_matches
import re
import spacy
from nltk.tokenize import word_tokenize

nlp = spacy.load("tr_core_news_trf")

insansi_eylemler = {
    "duy", "sor", "sus", "git", "oyna", "hazÄ±rla", "paylaÅŸ", "tut", "Ã¶r", "sÃ¶yle", "de", "konuÅŸ", 
    "dÃ¼ÅŸÃ¼n", "hisset", "hissed", "koyul", "kazan", "dokun", "tat", "yÃ¼rÃ¼", "koÅŸ", "uyu", "uyan", 
    "gÃ¼lÃ¼mse", "aÄŸla", "gÃ¼l", "sev", "kork", "Ã¶zlem", "iste", "hatÄ±rla", "unut", "kÄ±z", "Ã¶fkelen", 
    "Ã¶fke", "eÄŸit", "uyar", "heyecanlan", "hÃ¼zÃ¼nlen", "endiÅŸelen", "sevin", "Ã¼zÃ¼l", "merak", "meraklan", 
    "kÄ±skan", "ilgil", "sabÄ±rsÄ±zlan", "sakinleÅŸ", "kandÄ±r", "inandÄ±r", "cesaretlendir", "et", "incele", 
    "anla", "farket", "duygulan", "gÃ¼ven", "ÅŸÃ¼phelen", "ÅŸÃ¼phe", "destekle", "yargÄ±la", "suÃ§la", 
    "affet", "umutla", "eÄŸlen", "rahatla", "Ã§abala", "dÃ¼ÅŸle", "ÅŸaÅŸÄ±r", "ÅŸaÅŸ", "ÅŸevklen", "iste", 
    "koru", "gÃ¼l", "ver", "yap", "bak", "Ã¶ÄŸren", "Ã¶zle", "dinle", "planla", "arzula", "hedefle", "baÅŸla","sor",
    "gÃ¶zlemle", "Ã§Ã¶z", "kararlaÅŸtÄ±r", "reddet", "dinlen", "korkut", "izle", "oku", "anlat", "Ã§alÄ±ÅŸ","katÄ±l",
    "yaz", "tartÄ±ÅŸ", "hissizleÅŸ", "yÃ¼rekglen", "yÃ¼reklendir", "gizle", "dile", "anlamlandÄ±r", "Ã¶v", 
    "dÃ¶v", "eleÅŸtir", "sorgula", "haket", "diren", "vazgeÃ§", "anlaÅŸ", "baÄŸÄ±ÅŸla", "baÄŸÄ±r", "mÄ±rÄ±ldan","ye", 
    "Ã§alÄ±ÅŸ", "konuÅŸ", "al", "dile", "kÄ±z", "kÄ±r", "kÄ±rÄ±l", "hastalan"
}

turkce_stopwords = {
    "bir", "bu", "ÅŸu", "Ã§ok", "daha", "mi", "mu", "mÄ±", "mÃ¼", "ÅŸey", "tane",
    "ve","ile", "de", "da", "ki", "ne", "ni", "iÃ§in", "ama", "fakat", "ancak",
    "gibi", "ise", "en", "ya", "iÃ§", "herkes","kimse","kendi"," ","\"","â€œ","â€œ",
    "hem", "veya", "ya da", "Ã§Ã¼nkÃ¼", "lakin", "diye", "kadar", "sonra", "kiÅŸi",
    "Ã¶nce", "her", "hiÃ§", "hep", "bazÄ±", "tÃ¼m", "hangi", "neden", "nasÄ±l", "nerede", "ne zaman",
    "a", "na", "dan", "den", "e", "ye", "deki", "daki", "nin", "nÄ±n", "nun", "nÃ¼n",
    "ben", "sen", "o", "ora", "bura", "ÅŸura", "orada", "burada", "ÅŸurada", "biri", "biz", "siz"
}

def metin_onisleme(text):
    text = text.replace("â€™", "'").replace("â€œ", '"').replace("â€", '"')
    text = re.sub(r"'(\s+)", "'", text)
    return text

def temizle_kelime(kelime):
    return re.split(r"[â€™']", kelime)[0].lower()

def tamlayici_zincir(token):
    tamlayicilar = [token]
    stack = [token]
    while stack:
        current = stack.pop()
        for child in current.children:
            if child.dep_ in ["amod", "compound", "nmod:poss", "nmod"]:
                tamlayicilar.append(child)
                stack.append(child)
    if len(tamlayicilar) == 1:
        return None
    tamlayicilar = sorted(tamlayicilar, key=lambda x: x.i)
    return " ".join(t.text.lower() for t in tamlayicilar)

# ---------- Dosya oku ve iÅŸleme ----------
path = "NLP-Hikayeler/hikayeler/habilkabil.txt"
with open(path, "r", encoding="utf-8") as file:
    sentence = file.read()

sentence = metin_onisleme(sentence)
doc = nlp(sentence)

characters = set()

# NER tabanlÄ± karakter Ã§Ä±karÄ±mÄ±
for ent in doc.ents:
    temiz_karakter = temizle_kelime(ent.text)
    if ent.label_ == "PERSON" and temiz_karakter not in characters and len(ent) > 1:
        if temiz_karakter not in turkce_stopwords:
            characters.add(temiz_karakter)

# Fiil tabanlÄ± karakter Ã§Ä±karÄ±mÄ±
for token in doc:
    if token.dep_ in ["nsubj", "nsubjpass"] and token.pos_ in ["NOUN", "PROPN", "ADJ"]:
        birlesik = tamlayici_zincir(token)
        if birlesik:
            if token.head.pos_ == "VERB" and token.head.lemma_ in insansi_eylemler and all(k not in turkce_stopwords for k in birlesik.split()):
                characters.add(birlesik)
        else:
            temiz_karakter = temizle_kelime(token.lemma_)
            if token.head.pos_ == "VERB" and token.head.lemma_ in insansi_eylemler and temiz_karakter not in turkce_stopwords and len(temiz_karakter) > 1:
                characters.add(temiz_karakter)

# Karakter sayma
karakter_sayaci = Counter()
for character in characters:
    pattern = rf"\b{re.escape(character)}[a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼]*\b"
    matches = re.findall(pattern, sentence.lower())
    karakter_sayaci[character] += len(matches)

# Benzerleri grupla
karakter_birlesmis = set()
karakter_final = {}
filtered_characters = [k for k, v in karakter_sayaci.items() if v >= 5]

for karakter in filtered_characters:
    if karakter in karakter_birlesmis:
        continue
    benzerler = get_close_matches(karakter, filtered_characters, n=100, cutoff=0.8)
    toplam = sum(karakter_sayaci[b] for b in benzerler)
    en_fazla = max(benzerler, key=lambda x: karakter_sayaci[x])
    karakter_final[en_fazla] = toplam
    karakter_birlesmis.update(benzerler)

karakter_final_saf = {}
karakterler_sorted = sorted(karakter_final.items(), key=lambda x: -len(x[0]))
for i, (karakter, sayi) in enumerate(karakterler_sorted):
    alt_parca_mi = False
    for j in range(i):
        if karakter in karakterler_sorted[j][0]:
            alt_parca_mi = True
            break
    if not alt_parca_mi:
        karakter_final_saf[karakter] = sayi

print("\nMasalda GeÃ§en Karakterler ve GeÃ§iÅŸ SayÄ±larÄ±:")
for karakter, sayi in sorted(karakter_final_saf.items(), key=lambda x: x[1], reverse=True):
    print(f"{karakter}: {sayi} kez geÃ§ti")

with open("karakterler.txt", "w", encoding="utf-8") as file:
    for karakter in karakter_final_saf.keys():
        file.write(karakter + "\n")

# -------- Ã‡Ã–ZÃœM 1: Fiil tabanlÄ± karakter gruplama --------
karakter_eylemleri = defaultdict(set)

for sent in doc.sents:
    sent_text = sent.text.lower()
    for karakter in karakter_final_saf.keys():
        # Karakter cÃ¼mlede geÃ§iyor mu (kÃ¶k veya kÃ¶ke yakÄ±n biÃ§imiyle)
        if re.search(rf"\b{re.escape(karakter)}[a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼]*\b", sent_text):
            for token in sent:
                if token.pos_ == "VERB" and token.lemma_ in insansi_eylemler:
                    karakter_eylemleri[karakter].add(token.lemma_)

# SonuÃ§larÄ± yazdÄ±r
print("\nKarakterlerin geÃ§tiÄŸi cÃ¼mlelerdeki fiiller:")
for karakter, fiiller in karakter_eylemleri.items():
    print(f"\nğŸ§ {karakter}: {', '.join(sorted(fiiller))}")

# Benzer fiil iÃ§eren karakter gruplarÄ± (isteÄŸe baÄŸlÄ±)
benzer_gruplar = []
ziyaret_edilen = set()
karakter_listesi = list(karakter_eylemleri.items())

for i in range(len(karakter_listesi)):
    ad1, eylemler1 = karakter_listesi[i]
    if ad1 in ziyaret_edilen:
        continue
    grup = {ad1}
    for j in range(i + 1, len(karakter_listesi)):
        ad2, eylemler2 = karakter_listesi[j]
        if len(eylemler1 & eylemler2) >= 5:
            grup.add(ad2)
            ziyaret_edilen.add(ad2)
    if len(grup) > 1:
        benzer_gruplar.append(grup)
        ziyaret_edilen.update(grup)

print("\n--- Fiil tabanlÄ± benzer karakter gruplarÄ± ---")
for i, grup in enumerate(benzer_gruplar, 1):
    print(f"Grup {i}: {', '.join(grup)}")

with open("benzer_karakter_gruplari.txt", "w", encoding="utf-8") as f:
    for i, grup in enumerate(benzer_gruplar, 1):
        f.write(f"Grup {i}: {', '.join(grup)}\n")